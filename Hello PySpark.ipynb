{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "This is an example implementation of PageRank. For more conventional use,\n",
    "Please refer to PageRank implementation provided by graphx\n",
    "Example Usage:\n",
    "bin/spark-submit examples/src/main/python/pagerank.py data/mllib/pagerank_data.txt 10\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: This is a naive implementation of PageRank and is given as an example!\n",
      "Please refer to PageRank implementation provided by graphx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 has rank: 1.73800730412.\n",
      "4 has rank: 0.753997565294.\n",
      "3 has rank: 0.753997565294.\n",
      "2 has rank: 0.753997565294.\n"
     ]
    }
   ],
   "source": [
    "print(\"WARN: This is a naive implementation of PageRank and is given as an example!\\n\" +\n",
    "         \"Please refer to PageRank implementation provided by graphx\",\n",
    "          file=sys.stderr)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "spark = SparkSession\\\n",
    "      .builder\\\n",
    "        .appName(\"PythonPageRank\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Loads in input file. It should be in format of:\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     ...\n",
    "lines = spark.read.text(\"pagerank_data.txt\").rdd.map(lambda r: r[0])\n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(10):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank:rank*0.85+0.15);\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
